---
phase: 07-compute-expansion
plan: 03
type: execute
depends_on: ["07-02"]
files_modified: [core/inference/worker/agent.go, core/inference/cmd/orion-inference-worker/main.go, core/inference/cmd/orion-inference-router/main.go, bus/contracts/inference.request.schema.json, bus/contracts/inference.response.schema.json]
---

<objective>
Complete worker agent, create service binaries, and add integration tests.

Purpose: Tie together worker and router components into runnable services with JSON Schema contracts and comprehensive testing.
Output: Two executable binaries (orion-inference-worker, orion-inference-router) with contract validation and integration tests.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/07-compute-expansion/07-CONTEXT.md
@.planning/phases/07-compute-expansion/07-RESEARCH.md
@.planning/phases/07-compute-expansion/07-01-SUMMARY.md
@.planning/phases/07-compute-expansion/07-02-SUMMARY.md
@edge/cmd/orion-edge/main.go
@bus/go/cmd/orion-bus/main.go

**Existing patterns:**
- edge/cmd/orion-edge/main.go: Graceful shutdown, health endpoint, config via flags
- bus/go/cmd/orion-bus/main.go: Redis client setup, contract validation

**ORION contract requirements (from CLAUDE.md):**
- All inputs validated against contracts
- Validate at module boundaries
- JSON Schema for language-agnostic validation
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement inference worker agent with Ollama integration</name>
  <files>core/inference/worker/agent.go, core/inference/worker/agent_test.go</files>
  <action>
Create worker agent that processes inference requests using Ollama:

1. Create WorkerAgent struct:
   - Fields: nodeID, ollamaHost string, redis *redis.Client, metrics *HealthCollector, registry *HealthRegistry
   - Constructor: NewWorkerAgent(nodeID, ollamaHost string, redis *redis.Client)
   - Initialize Ollama client via api.ClientFromEnvironment() or api.NewClient(url)

2. Implement Start(ctx context.Context) error:
   - Start health publisher goroutine (every 5 seconds)
   - Start request consumer goroutine
   - Block until context cancelled

3. Implement publishHealthLoop(ctx context.Context):
   - Collect health via metrics.CollectHealth()
   - Publish to registry.PublishHealth()
   - Log errors but don't fail (resilient to temporary issues)
   - Use time.Ticker with 5 second interval

4. Implement consumeRequests(ctx context.Context) error:
   - Create consumer group on "orion:inference:requests:{nodeID}"
   - Read via XReadGroup with 1 second block
   - For each request: call processInference()
   - ACK processed messages

5. Implement processInference(ctx context.Context, req InferenceRequest) error:
   - Convert InferenceRequest to api.ChatRequest
   - Set KeepAlive from req.KeepAlive (default 10 minutes)
   - Call ollama.Chat(ctx, req, callback) for streaming
   - Collect full response
   - Build InferenceResponse with timing metrics
   - Publish response to req.Callback stream

6. Implement Stop() error:
   - Remove from health registry
   - Close connections gracefully

7. Write unit tests (mock Ollama with interface):
   - Test health publishing loop
   - Test request consumption
   - Test graceful shutdown removes from registry

AVOID: Direct HTTP calls to Ollama (use official api package). Don't block on slow inference (use streaming).
  </action>
  <verify>go test ./worker/... -v passes agent tests, health loop and request processing work</verify>
  <done>WorkerAgent implemented with Ollama integration, health publishing, and request consumption</done>
</task>

<task type="auto">
  <name>Task 2: Create JSON Schema contracts and service binaries</name>
  <files>bus/contracts/inference.request.schema.json, bus/contracts/inference.response.schema.json, core/inference/cmd/orion-inference-worker/main.go, core/inference/cmd/orion-inference-router/main.go</files>
  <action>
Create contract schemas and executable service binaries:

1. Create inference.request.schema.json:
```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "InferenceRequest",
  "type": "object",
  "required": ["version", "request_id", "model", "messages", "timestamp", "source"],
  "properties": {
    "version": {"type": "string", "const": "1.0"},
    "request_id": {"type": "string", "format": "uuid"},
    "model": {"type": "string", "minLength": 1},
    "messages": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["role", "content"],
        "properties": {
          "role": {"enum": ["user", "assistant", "system"]},
          "content": {"type": "string"}
        }
      },
      "minItems": 1
    },
    "keep_alive_seconds": {"type": "integer", "minimum": 0, "default": 600},
    "callback": {"type": "string"},
    "timestamp": {"type": "string", "format": "date-time"},
    "source": {"type": "string"}
  }
}
```

2. Create inference.response.schema.json:
```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "InferenceResponse",
  "type": "object",
  "required": ["version", "request_id", "model", "timestamp", "source"],
  "properties": {
    "version": {"type": "string", "const": "1.0"},
    "request_id": {"type": "string", "format": "uuid"},
    "model": {"type": "string"},
    "response": {"type": "string"},
    "prompt_tokens": {"type": "integer", "minimum": 0},
    "completion_tokens": {"type": "integer", "minimum": 0},
    "load_duration_ms": {"type": "integer", "minimum": 0},
    "total_duration_ms": {"type": "integer", "minimum": 0},
    "error": {"type": "string"},
    "timestamp": {"type": "string", "format": "date-time"},
    "source": {"type": "string"}
  }
}
```

3. Create orion-inference-worker main.go:
   - Flags: --node-id, --redis-addr, --redis-password, --ollama-host, --http-port
   - Create WorkerAgent and start
   - HTTP health endpoint at /health (returns node health)
   - Graceful shutdown with 25s timeout (matches orion-bus pattern)
   - Log startup: "ORION Inference Worker v0.1.0 starting (node: %s)"

4. Create orion-inference-router main.go:
   - Flags: --redis-addr, --redis-password, --http-port
   - Create InferenceRouter and start consuming
   - HTTP health endpoint at /health
   - Graceful shutdown
   - Log startup: "ORION Inference Router v0.1.0 starting"

5. Add Makefile targets:
   - make build-worker: builds orion-inference-worker
   - make build-router: builds orion-inference-router
   - make build: builds both

Follow patterns from edge/cmd/orion-edge/main.go and bus/go/cmd/orion-bus/main.go.
  </action>
  <verify>go build ./cmd/... produces both binaries, schemas validate with jsonschema lint</verify>
  <done>Contract schemas created, both service binaries compile and run with --help</done>
</task>

<task type="auto">
  <name>Task 3: Add integration tests and documentation</name>
  <files>core/inference/integration_test.go, docs/INFERENCE-SETUP.md</files>
  <action>
Create integration tests and setup documentation:

1. Create integration_test.go with //go:build integration tag:
   - Test worker publishes health to Redis (use miniredis)
   - Test router reads health and routes to correct stream
   - Test sticky routing prefers node with model resident
   - Test fallback routing when model not resident
   - Test health threshold enforcement (temp > 75, RAM > 90)

2. Test scenarios:
   a. TestWorkerHealthPublishing:
      - Start worker with mock Ollama
      - Wait for health publish (5s interval)
      - Verify Redis hash contains correct health data

   b. TestRouterStickyRouting:
      - Seed health registry with 2 nodes (one with model, one without)
      - Submit inference request for that model
      - Verify request dispatched to node with model

   c. TestRouterFallbackRouting:
      - Seed health registry with 2 nodes (neither has model)
      - Submit inference request
      - Verify request dispatched to least-loaded node

   d. TestHealthThresholdEnforcement:
      - Seed node with temp=80°C
      - Verify node excluded from routing
      - Seed node with RAM=95%
      - Verify node excluded from routing

3. Create docs/INFERENCE-SETUP.md:
   - Architecture overview diagram (ASCII)
   - Prerequisites (Ollama installed, Redis running)
   - Build instructions
   - Configuration options (all flags documented)
   - Deployment guide (systemd service examples)
   - Health endpoint documentation
   - Troubleshooting guide
   - Redis stream names reference

4. Update core/inference/README.md:
   - Module purpose
   - Inputs/outputs
   - Invariants (sticky routing, health thresholds)
   - Failure modes (no nodes available, Ollama unreachable)

Integration test pattern from edge/integration_test.go: Use build tags to separate from unit tests.
  </action>
  <verify>go test -tags=integration ./... passes, docs/INFERENCE-SETUP.md exists and is complete</verify>
  <done>Integration tests pass, documentation complete, Phase 7 ready for deployment</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `go build ./cmd/...` produces orion-inference-worker and orion-inference-router
- [ ] `go test ./...` passes all unit tests
- [ ] `go test -tags=integration ./...` passes integration tests
- [ ] JSON Schema files validate with jsonschema lint
- [ ] docs/INFERENCE-SETUP.md covers deployment and troubleshooting
- [ ] Both binaries start and show health endpoint
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Service binaries run and expose health endpoints
- Contracts follow ORION patterns (version, timestamp, source fields)
- Integration tests verify sticky routing and health thresholds
- Documentation sufficient for deployment
- Phase 7: Compute Expansion complete
</success_criteria>

<output>
After completion, create `.planning/phases/07-compute-expansion/07-03-SUMMARY.md`:

# Phase 7 Plan 03: Integration & Testing Summary

**[Substantive one-liner describing what shipped]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `path/to/file.go` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Phase 7 Complete

Phase 7: Compute Expansion complete. The inference subsystem provides:
- Health-aware distributed Ollama cluster
- Sticky routing for model residency optimization
- Safety backoff at temp > 75°C or RAM > 90%
- Redis Streams for async request/response
- Two deployable services (worker, router)

**Hardware roles confirmed:**
- Pi 16GB: Runs router + worker
- Pi 8GB: Runs worker only
- Pi 4GB: EXCLUDED (kinematics only)

**Test Summary:**
- `go test ./...` - Unit tests passing
- `go test -tags=integration ./...` - Integration tests passing

**Next Phase:** None defined (Phase 7 is final phase in current roadmap)
</output>
