---
phase: 05-ai-council
plan: 01
type: execute
depends_on: []
files_modified: [core/council/council_validator.py, core/council/memory_manager.py, core/council/__init__.py]
---

<objective>
Implement local SLM validation interface with resource monitoring for Raspberry Pi 5 constraints.

Purpose: Create CouncilValidator to interface with Ollama-served Gemma-2 2B model and MemoryManager to prevent OOM crashes by monitoring RAM and temperature before model loading.
Output: Working local SLM validator with hardware-aware safeguards.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-ai-council/05-CONTEXT.md
@.planning/phases/05-ai-council/05-RESEARCH.md
@.planning/codebase/ARCHITECTURE.md
@.planning/codebase/STACK.md
@.planning/codebase/TESTING.md

**Research findings:**
- Gemma-2 2B: 3GB RAM, Q4_0 quantization, 3-7 tokens/sec on Pi 5
- Ollama provides simplest API with Python client
- Sequential loading mandatory (no parallel models)
- Thermal management critical for sustained inference
- RAM monitoring: Don't load if < 4GB free

**Established patterns:**
- Python 3.12 with type hints throughout
- Dataclass-based contracts
- Comprehensive docstrings on public APIs
- pytest for testing with fakeredis
- Conservative by default, fail-closed on errors

**Safety constraints:**
- Never load multiple local models simultaneously
- Check RAM before loading (< 4GB free → block)
- Monitor temperature during inference (> 70°C → warn)
- Enforce 30-second timeout per validation
- Fail-closed: errors/timeouts → block action
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement CouncilValidator with Ollama integration</name>
  <files>core/council/council_validator.py, core/council/__init__.py</files>
  <action>
Create CouncilValidator class in core/council/council_validator.py:
- Use ollama Python client (add to requirements.txt: ollama==0.1.6)
- Implement validate(decision: Dict) -> Tuple[float, str] method
  - Accepts decision dict (incident context + Brain reasoning)
  - Constructs validation prompt asking model to:
    1. Evaluate if SAFE/RISKY classification correct per policies
    2. Assess if reasoning makes sense given incident
    3. Self-report confidence score (0.0-1.0)
  - Calls ollama.generate(model='gemma2:2b', prompt=..., timeout=30)
  - Parses response to extract confidence and critique
  - Returns (confidence_score, critique_text)
- Handle errors: ollama.exceptions.ResponseError → return (0.0, "ERROR: model unavailable")
- Handle timeout: raise TimeoutError after 30 seconds
- Use ollama library (NOT subprocess calls) for cleaner error handling
- Add comprehensive docstring explaining hardware constraints

Create __init__.py exposing CouncilValidator.

DO NOT implement confidence extraction yet (Task 2 will add it).
DO NOT add MemoryManager checks yet (Task 2 handles resource monitoring).
DO NOT use subprocess or shell commands - use ollama Python client exclusively.
  </action>
  <verify>
Import succeeds: python -c "from core.council import CouncilValidator; c = CouncilValidator(); print(type(c))"
Basic structure check: grep -q "def validate" core/council/council_validator.py
  </verify>
  <done>
CouncilValidator class exists with validate() method signature.
ollama library added to requirements.txt.
Imports work without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement MemoryManager for resource monitoring</name>
  <files>core/council/memory_manager.py, core/council/__init__.py</files>
  <action>
Create MemoryManager class in core/council/memory_manager.py:
- Use psutil library (already in requirements.txt: psutil==5.9.8)
- Implement check_resources_before_load() -> Tuple[bool, str] method
  - Check available RAM: psutil.virtual_memory().available / (1024**3) GB
  - If < 4GB free: return (False, "Insufficient RAM: {X}GB free, need 4GB minimum")
  - Check CPU temperature (Pi 5): Try vcgencmd measure_temp via subprocess (optional)
    - If > 70°C: log WARNING (don't block, just warn)
  - If all checks pass: return (True, "Resources OK")
- Implement monitor_during_inference() context manager
  - Tracks RAM usage during validation
  - Logs WARNING if temperature rises above 70°C
  - Uses time.time() to enforce overall timeout
- Add type hints throughout
- Add comprehensive docstring explaining Pi 5 constraints

Update __init__.py to expose MemoryManager.

Integrate MemoryManager into CouncilValidator.validate():
- Call check_resources_before_load() BEFORE ollama.generate()
- If insufficient resources: return (0.0, "BLOCKED: Insufficient resources")
- Wrap ollama.generate() in monitor_during_inference() context manager

Use psutil for RAM checks (already in stack).
Temperature monitoring is OPTIONAL (vcgencmd may not be available in all environments).
DO NOT fail if temperature check unavailable - log info and continue.
  </action>
  <verify>
Import succeeds: python -c "from core.council import MemoryManager; m = MemoryManager(); ok, msg = m.check_resources_before_load(); print(ok, msg)"
MemoryManager integrated: grep -q "check_resources_before_load" core/council/council_validator.py
  </verify>
  <done>
MemoryManager class exists with resource checking methods.
CouncilValidator calls check_resources_before_load() before model inference.
Temperature monitoring implemented (optional, doesn't block on failure).
RAM checks enforce 4GB minimum free memory.
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pytest tests/test_council_validator.py -v` passes (if tests written)
- [ ] No import errors from core.council
- [ ] MemoryManager correctly checks RAM and temperature
- [ ] CouncilValidator enforces resource checks before inference
</verification>

<success_criteria>

- CouncilValidator class implemented with Ollama integration
- MemoryManager enforces Pi 5 resource constraints
- RAM check blocks loading if < 4GB free
- Temperature monitoring logs warnings (non-blocking)
- 30-second timeout enforced
- ollama library added to requirements.txt
- All type hints present
- Comprehensive docstrings on public APIs
- No syntax errors, imports work
</success_criteria>

<output>
After completion, create `.planning/phases/05-ai-council/05-01-SUMMARY.md`:

# Phase 5 Plan 1: Local SLM Validator Summary

**Implemented CouncilValidator with Ollama integration and Pi 5-aware resource monitoring.**

## Accomplishments

- CouncilValidator interfaces with Ollama-served Gemma-2 2B model
- MemoryManager enforces 4GB minimum free RAM before loading
- Temperature monitoring implemented (optional, non-blocking)
- 30-second timeout per validation
- Fail-closed error handling

## Files Created/Modified

- `core/council/council_validator.py` - Local SLM validation interface
- `core/council/memory_manager.py` - Resource monitoring for Pi 5
- `core/council/__init__.py` - Module exports
- `requirements.txt` - Added ollama==0.1.6

## Decisions Made

- Gemma-2 2B chosen over Phi-3 (lower RAM: 3GB vs 5GB)
- Ollama Python client over subprocess calls (cleaner error handling)
- Temperature monitoring non-blocking (availability varies by environment)
- 4GB free RAM threshold (conservative, leaves margin for other services)

## Issues Encountered

[Document any issues, or "None"]

## Next Step

Ready for 05-02-PLAN.md (External Validator with Claude + OpenAI APIs)
</output>
