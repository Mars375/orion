---
phase: 05-ai-council
plan: 02
type: execute
depends_on: []
files_modified: [core/council/external_validator.py, core/council/__init__.py, requirements.txt]
---

<objective>
Implement external API validator for Claude and OpenAI with fail-closed behavior.

Purpose: Create ExternalValidator to call Claude 3.7 Sonnet and OpenAI GPT-4 Turbo APIs for high-confidence validation when local SLM is uncertain or decision is RISKY.
Output: Working external API validator with retry logic and fail-closed error handling.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-ai-council/05-CONTEXT.md
@.planning/phases/05-ai-council/05-RESEARCH.md
@.planning/codebase/ARCHITECTURE.md
@.planning/codebase/STACK.md

**Research findings:**
- Claude 3.7 Sonnet: Superior reasoning, 200K context, structured outputs
- OpenAI GPT-4 Turbo: Diverse perspective, tie-breaker
- Use native SDKs: anthropic-sdk-python, openai-sdk-python
- Fail-closed on timeout: treat as "BLOCKED"
- External APIs run in parallel (asyncio.gather)
- Cost optimization: only escalate if local uncertain or RISKY

**Established patterns:**
- Python 3.12 with type hints and async/await
- Fail-closed on errors (never fail-open)
- Conservative by default
- Environment variables for API keys
- Comprehensive error handling

**Safety constraints:**
- Timeout = fail-closed (return low confidence, not exception)
- Network errors = fail-closed
- API rate limits = fail-closed
- Missing API keys = fail-closed (log error)
- Never assume external API availability
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement ExternalValidator with Claude and OpenAI integration</name>
  <files>core/council/external_validator.py, core/council/__init__.py, requirements.txt</files>
  <action>
Create ExternalValidator class in core/council/external_validator.py:
- Add to requirements.txt: anthropic==0.18.1, openai==1.12.0
- Import anthropic, openai, asyncio libraries
- Constructor loads API keys from environment:
  - ANTHROPIC_API_KEY (optional)
  - OPENAI_API_KEY (optional)
  - If missing: log WARNING, continue (methods will fail-closed)
- Implement async validate_with_claude(decision: Dict) -> Tuple[float, str]
  - Construct validation prompt (same structure as local SLM)
  - Call anthropic.Anthropic().messages.create()
    - model="claude-3-5-sonnet-20241022"
    - max_tokens=1024
    - temperature=0.0 (deterministic)
    - timeout=10 seconds
  - Parse response to extract confidence and critique
  - Error handling: anthropic.APIError → return (0.0, "ERROR: Claude API unavailable")
  - Timeout: return (0.0, "ERROR: Claude API timeout")
- Implement async validate_with_openai(decision: Dict) -> Tuple[float, str]
  - Construct validation prompt
  - Call openai.OpenAI().chat.completions.create()
    - model="gpt-4-turbo"
    - max_tokens=1024
    - temperature=0.0
    - timeout=10 seconds
  - Parse response to extract confidence and critique
  - Error handling: openai.APIError → return (0.0, "ERROR: OpenAI API unavailable")
- Implement async validate_parallel(decision: Dict) -> List[Tuple[float, str]]
  - Calls both APIs in parallel using asyncio.gather()
  - Returns list of (confidence, critique) tuples from both APIs
  - If API key missing: skip that API, log INFO
  - If both keys missing: return [(0.0, "ERROR: No external APIs configured")]
- Add type hints throughout
- Add comprehensive docstring explaining fail-closed behavior

Update __init__.py to expose ExternalValidator.

Use official SDKs (NOT requests library or custom HTTP calls).
DO NOT raise exceptions on API errors - always return (0.0, error_message).
DO NOT implement confidence extraction yet (Plan 03 will handle parsing).
  </action>
  <verify>
Import succeeds: python -c "from core.council import ExternalValidator; e = ExternalValidator(); print(type(e))"
Async methods present: grep -q "async def validate_with_claude" core/council/external_validator.py
  </verify>
  <done>
ExternalValidator class exists with async validation methods.
anthropic and openai libraries added to requirements.txt.
validate_parallel() calls both APIs in parallel.
Fail-closed error handling for all API failures.
Missing API keys handled gracefully (no exceptions).
  </done>
</task>

<task type="auto">
  <name>Task 2: Add retry logic with exponential backoff</name>
  <files>core/council/external_validator.py</files>
  <action>
Add retry logic to ExternalValidator:
- Implement _retry_with_backoff() helper method
  - Parameters: async_func, max_retries=2, initial_delay=1.0
  - Exponential backoff: delay *= 2 after each retry
  - Retries on: anthropic.APIConnectionError, openai.APIConnectionError
  - Does NOT retry on: anthropic.AuthenticationError, openai.AuthenticationError (fail immediately)
  - Does NOT retry on: anthropic.RateLimitError (fail-closed, log WARNING)
  - Returns (0.0, error_message) if all retries exhausted
- Update validate_with_claude() to wrap API call in _retry_with_backoff()
- Update validate_with_openai() to wrap API call in _retry_with_backoff()
- Add logging: INFO on retry, WARNING on final failure
- Add docstring explaining retry strategy

DO NOT retry indefinitely - max 2 retries (total 3 attempts).
DO NOT retry authentication errors (misconfiguration, not transient).
DO NOT retry rate limit errors (fail-closed, don't hammer API).
Use asyncio.sleep() for backoff delays.
  </action>
  <verify>
Retry logic present: grep -q "_retry_with_backoff" core/council/external_validator.py
Max retries configurable: grep -q "max_retries" core/council/external_validator.py
  </verify>
  <done>
_retry_with_backoff() method implemented with exponential backoff.
validate_with_claude() and validate_with_openai() use retry logic.
Max 2 retries (3 total attempts) for connection errors.
Authentication and rate limit errors fail immediately.
Logging added for retries and failures.
  </done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pytest tests/test_external_validator.py -v` passes (if tests written)
- [ ] No import errors from core.council
- [ ] ExternalValidator handles missing API keys gracefully
- [ ] Retry logic only retries transient errors
- [ ] All errors return (0.0, error_message), never raise
</verification>

<success_criteria>

- ExternalValidator class implemented with Claude + OpenAI
- validate_parallel() calls both APIs concurrently
- Retry logic with exponential backoff (max 2 retries)
- Fail-closed on all errors (never fail-open)
- Missing API keys handled gracefully
- anthropic==0.18.1 and openai==1.12.0 added to requirements.txt
- All type hints present
- Comprehensive docstrings
- No syntax errors, imports work
</success_criteria>

<output>
After completion, create `.planning/phases/05-ai-council/05-02-SUMMARY.md`:

# Phase 5 Plan 2: External API Validator Summary

**Implemented ExternalValidator with Claude and OpenAI API integration, retry logic, and fail-closed error handling.**

## Accomplishments

- ExternalValidator interfaces with Claude 3.7 Sonnet and OpenAI GPT-4 Turbo
- validate_parallel() runs both APIs concurrently
- Retry logic with exponential backoff (max 2 retries)
- Fail-closed on all errors (network, auth, rate limit, timeout)
- Missing API keys handled gracefully

## Files Created/Modified

- `core/council/external_validator.py` - External API validation interface
- `core/council/__init__.py` - Module exports
- `requirements.txt` - Added anthropic==0.18.1, openai==1.12.0

## Decisions Made

- Claude 3.7 Sonnet chosen (superior reasoning per research)
- Parallel API calls via asyncio.gather (2-5 seconds total)
- Max 2 retries with exponential backoff (connection errors only)
- Fail-closed on rate limits (don't hammer APIs)
- Authentication errors fail immediately (misconfiguration)

## Issues Encountered

[Document any issues, or "None"]

## Next Step

Ready for 05-03-PLAN.md (Consensus Aggregator with confidence-weighted voting)
</output>
